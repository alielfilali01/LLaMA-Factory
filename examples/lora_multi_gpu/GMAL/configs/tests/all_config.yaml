# LLaMA-Factory Configuration File

# General configuration
model_name_or_path: "Qwen/Qwen1.5-7B"  # Path to the pre-trained model or model identifier from Hugging Face Hub
adapter_name_or_path: ""  # Optional: Path to the adapter model if used
cache_dir: ""  # Directory to store the pre-trained models downloaded from Hugging Face Hub
use_fast_tokenizer: true  # Whether to use a fast tokenizer backed by the 'tokenizers' library
resize_vocab: false  # Whether to resize the vocabulary
split_special_tokens: false  # Whether to split special tokens
new_special_tokens: ""  # New special tokens to add to the tokenizer
model_revision: "main"  # The specific model version to use (can be a branch name, tag name, or commit id)
low_cpu_mem_usage: true  # Use memory-efficient model loading
quantization_bit: 4  # Number of bits for quantization (e.g., 8, 4)
quantization_type: "fp4"  # Type of quantization ('fp4', 'nf4')
double_quantization: false  # Whether to use double quantization
quantization_device_map: "auto"  # Device map for quantization
rope_scaling: "linear"  # Type of ROPE scaling ('linear', 'dynamic')
flash_attn: "auto"  # Enable FlashAttention for faster training and inference ('off', 'sdpa', 'fa2', 'auto')
shift_attn: false  # Whether to shift attention
mixture_of_depths: "load"  # Mixture of depths mode ('convert', 'load')
use_unsloth: false  # Use Unsloth
visual_inputs: false  # Use visual inputs
moe_aux_loss_coef: 0.1  # Mixture of experts auxiliary loss coefficient
disable_gradient_checkpointing: false  # Disable gradient checkpointing
upcast_layernorm: false  # Upcast layer normalization
upcast_lmhead_output: false  # Upcast LM head output
infer_backend: "huggingface"  # Inference backend ('huggingface', 'vllm')
vllm_maxlen: 4096  # Maximum length for vLLM
vllm_gpu_util: 0.8  # GPU utilization for vLLM
vllm_enforce_eager: false  # Enforce eager execution for vLLM
vllm_max_lora_rank: 64  # Maximum LoRA rank for vLLM
offload_folder: ""  # Directory for offloading
use_cache: true  # Whether to use cache
hf_hub_token: ""  # Hugging Face Hub token
ms_hub_token: ""  # Microsoft Hub token

# Export configuration
export_dir: ""  # Directory to export the model
export_size: 2  # Export size
export_device: "cpu"  # Device for export ('cpu', 'cuda')
export_quantization_bit: 8  # Number of bits for quantization during export
export_quantization_dataset: ""  # Dataset for export quantization
export_quantization_nsamples: 1000  # Number of samples for export quantization
export_quantization_maxlen: 1024  # Maximum length for export quantization
export_legacy_format: false  # Use legacy format for export
export_hub_model_id: ""  # Model ID for exporting to Hugging Face Hub

# Data configuration
dataset: "AbderrahmanSkiredj1-wiki-ary"  # Name of the dataset
dataset_dir: "./data/"  # Directory containing the dataset
split: "train"  # Dataset split to use ('train', 'eval', 'test')
cutoff_len: 4096  # Maximum length of the input sequence
reserved_label_len: 0  # Reserved length for labels
train_on_prompt: false  # Train on prompt
streaming: false  # Use streaming for data loading
buffer_size: 100  # Buffer size for streaming
mix_strategy: "concat"  # Strategy to mix datasets ('concat', 'interleave_under', 'interleave_over')
interleave_probs: 1.0  # Interleave probabilities for datasets
overwrite_cache: true  # Overwrite the cached preprocessed datasets
preprocessing_num_workers: 16  # Number of workers for data preprocessing
max_samples: 1000  # Maximum number of samples to use

# Evaluation configuration
eval_num_beams: 5  # Number of beams for evaluation
ignore_pad_token_for_loss: true  # Ignore pad tokens in loss computation
val_size: 0.1  # Validation size
packing: false  # Use packing for evaluation
tokenized_path: ""  # Path to tokenized data

# Training configuration
output_dir: "./LLaMA-Factory/saves/qwen-7b-lora-ary-wiki/"  # Output directory
overwrite_output_dir: true  # Overwrite the content of the output directory
do_train: true  # Whether to run training
do_eval: true  # Whether to run evaluation
do_predict: false  # Whether to run prediction
eval_strategy: "steps"  # Evaluation strategy ('no', 'steps', 'epoch')
prediction_loss_only: false  # Only return the loss during evaluation
per_device_train_batch_size: 4  # Batch size per device during training
per_device_eval_batch_size: 1  # Batch size per device during evaluation
gradient_accumulation_steps: 4  # Number of gradient accumulation steps
eval_accumulation_steps: 4  # Number of evaluation accumulation steps
eval_delay: 0  # Delay between evaluations
learning_rate: 5e-6  # Initial learning rate
weight_decay: 0.1  # Weight decay
adam_beta1: 0.9  # Beta1 for Adam optimizer
adam_beta2: 0.999  # Beta2 for Adam optimizer
adam_epsilon: 1e-8  # Epsilon for Adam optimizer
max_grad_norm: 1.0  # Maximum gradient norm
num_train_epochs: 3.0  # Number of training epochs
max_steps: -1  # Maximum number of training steps
lr_scheduler_type: "cosine"  # Type of learning rate scheduler
warmup_ratio: 0.1  # Ratio of warmup steps
warmup_steps: 100  # Number of warmup steps
log_level: "info"  # Logging level
log_level_replica: "info"  # Logging level for replicas
log_on_each_node: true  # Log on each node
logging_dir: "./logs/"  # Directory for logging
logging_strategy: "steps"  # Logging strategy ('no', 'steps', 'epoch')
logging_first_step: true  # Log the first step
logging_steps: 1  # Number of steps between logging
logging_nan_inf_filter: true  # Filter NaN and Inf values from logs
save_strategy: "steps"  # Save strategy ('no', 'steps', 'epoch')
save_steps: 1000  # Number of steps between saves
save_total_limit: 5  # Total limit of saved checkpoints
save_safetensors: true  # Save in safetensors format
save_on_each_node: true  # Save on each node
save_only_model: true  # Only save the model
restore_callback_states_from_checkpoint: true  # Restore callback states from checkpoint

# Hardware configuration
no_cuda: false  # Do not use CUDA even if it is available
use_cpu: false  # Use CPU for training
use_mps_device: false  # Use MPS device for training
seed: 42  # Random seed for initialization
data_seed: 42  # Random seed for data
jit_mode_eval: false  # Use JIT mode for evaluation
use_ipex: false  # Use Intel PyTorch Extension (IPEX)
bf16: true  # Use bfloat16 precision
fp16: false  # Use float16 precision
fp16_opt_level: "O1"  # Optimization level for mixed precision training
half_precision_backend: "auto"  # Backend for half precision ('auto', 'apex', 'cpu_amp')
bf16_full_eval: true  # Use bfloat16 precision for full evaluation
fp16_full_eval: false  # Use float16 precision for full evaluation
tf32: true  # Use TF32 for training
local_rank: -1  # Local rank for distributed training
ddp_backend: "nccl"  # Backend for distributed data parallel training ('nccl', 'gloo', 'mpi', 'ccl', 'hccl', 'cncl')
tpu_num_cores: 8  # Number of TPU cores to use
tpu_metrics_debug: false  # Enable TPU metrics debugging
debug: []  # Debug options

# Dataloader configuration
dataloader_drop_last: true  # Drop the last incomplete batch
eval_steps: 500  # Number of steps between evaluations
dataloader_num_workers: 16  # Number of workers for data loading
dataloader_prefetch_factor: 2  # Prefetch factor for data loading
past_index: -1  # Index of the past state for caching
run_name: "qwen_training_run"  # Name of the training run
disable_tqdm: false  # Disable TQDM progress bar
remove_unused_columns: true  # Remove unused columns from the dataset
label_names: ["labels"]  # List of label names
load_best_model_at_end: true  # Load the best model at the end of training
metric_for_best_model: "accuracy"  # Metric to use for selecting the best model
greater_is_better: true  # Whether the metric should be maximized

# FSDP configuration
fsdp: ""  # Fully Sharded Data Parallel (FSDP) configuration
fsdp_min_num_params: 1e8  # Minimum number of parameters for FSDP
fsdp_config: ""  # Configuration for FSDP
fsdp_transformer_layer_cls_to_wrap: ""  # Transformer layer classes to wrap for FSDP

# Accelerator configuration
accelerator_config: ""  # Configuration for accelerator
deepspeed: ""  # Path to the DeepSpeed configuration file
label_smoothing_factor: 0.1  # Label smoothing factor

# Optimizer configuration
optim: "adamw_hf"  # Optimizer to use
optim_args: ""  # Additional arguments for the optimizer
adafactor: false  # Use Adafactor optimizer
group_by_length: false  # Group sequences of similar lengths
length_column_name: "length"  # Column name for sequence lengths
report_to: ["wandb"]  # Reporting tools to use

# DDP configuration
ddp_find_unused_parameters: false  # Find unused parameters in DDP
ddp_bucket_cap_mb: 25  # Bucket capacity for DDP
ddp_broadcast_buffers: true  # Broadcast buffers in DDP

# DataLoader configuration
dataloader_pin_memory: true  # Pin memory for DataLoader
dataloader_persistent_workers: true  # Persistent workers for DataLoader
skip_memory_metrics: true  # Skip memory metrics
use_legacy_prediction_loop: false  # Use legacy prediction loop

# Hugging Face Hub configuration
push_to_hub: true  # Push the model to Hugging Face Hub
resume_from_checkpoint: ""  # Resume training from checkpoint
hub_model_id: ""  # Model ID for Hugging Face Hub
hub_strategy: "every_save"  # Strategy for pushing to Hugging Face Hub ('end', 'every_save', 'checkpoint', 'all_checkpoints')
hub_token: ""  # Token for Hugging Face Hub
hub_private_repo: true  # Use a private repository on Hugging Face Hub
hub_always_push: false  # Always push to Hugging Face Hub

# Gradient checkpointing configuration
gradient_checkpointing: true  # Enable gradient checkpointing
gradient_checkpointing_kwargs: ""  # Additional arguments for gradient checkpointing

# Additional configuration

include_inputs_for_metrics: false  # Include inputs for metrics
eval_do_concat_batches: true  # Concatenate batches during evaluation
fp16_backend: "auto"  # Backend for FP16 ('auto', 'apex', 'cpu_amp')
evaluation_strategy: "steps"  # Evaluation strategy ('no', 'steps', 'epoch')
push_to_hub_model_id: ""  # Model ID for pushing to Hugging Face Hub
push_to_hub_organization: "Ali-C137"  # Organization for pushing to Hugging Face Hub
push_to_hub_token: ""  # Token for pushing to Hugging Face Hub
mp_parameters: ""  # Mixed precision parameters
auto_find_batch_size: true  # Automatically find batch size
full_determinism: true  # Enable full determinism
torchdynamo: ""  # TorchDynamo configuration
ray_scope: ""  # Ray scope configuration
ddp_timeout: 9000  # DDP timeout in seconds
torch_compile: true  # Enable Torch compile
torch_compile_backend: "inductor"  # Backend for Torch compile
torch_compile_mode: "default"  # Mode for Torch compile
dispatch_batches: true  # Dispatch batches
split_batches: true  # Split batches
include_tokens_per_second: true  # Include tokens per second in metrics
include_num_input_tokens_seen: true  # Include number of input tokens seen in metrics
neftune_noise_alpha: 0.0  # Noise alpha for NeFTuNE
optim_target_modules: ""  # Target modules for optimizer
batch_eval_metrics: true  # Batch evaluation metrics
sortish_sampler: true  # Use sortish sampler
predict_with_generate: true  # Predict with generate
generation_max_length: 1024  # Maximum length for generation
generation_num_beams: 5  # Number of beams for generation
generation_config: ""  # Configuration for generation
use_badam: false  # Use BAdam optimizer
badam_mode: "layer"  # Mode for BAdam optimizer
badam_start_block: 0  # Start block for BAdam optimizer
badam_switch_mode: "fixed"  # Switch mode for BAdam optimizer
badam_switch_interval: 1000  # Switch interval for BAdam optimizer
badam_update_ratio: 0.1  # Update ratio for BAdam optimizer
badam_mask_mode: "adjacent"  # Mask mode for BAdam optimizer
badam_verbose: false  # Verbose mode for BAdam optimizer
use_galore: false  # Use Galore optimizer
galore_target: ""  # Target for Galore optimizer
galore_rank: 0  # Rank for Galore optimizer
galore_update_interval: 1000  # Update interval for Galore optimizer
galore_scale: 1.0  # Scale for Galore optimizer
galore_proj_type: "std"  # Projection type for Galore optimizer
galore_layerwise: false  # Layerwise mode for Galore optimizer
dpo_beta: 1.0  # Beta for DPO
dpo_loss: "sigmoid"  # Loss function for DPO
dpo_label_smoothing: 0.1  # Label smoothing for DPO
dpo_ftx: false  # Use FTX for DPO
kto_beta: 1.0  # Beta for KTO
kto_chosen_weight: 1.0  # Chosen weight for KTO
kto_rejected_weight: 0.5  # Rejected weight for KTO
kto_ftx: false  # Use FTX for KTO
orpo_beta: 1.0  # Beta for ORPO
ppo_buffer_size: 1024  # Buffer size for PPO
ppo_epochs: 3  # Number of epochs for PPO
ppo_score_norm: true  # Normalize scores for PPO
ppo_target: ""  # Target for PPO
ppo_whiten_rewards: true  # Whiten rewards for PPO
ref_model: ""  # Reference model
ref_model_adapters: ""  # Adapters for reference model
ref_model_quantization_bit: 8  # Number of bits for quantization of reference model
reward_model: ""  # Reward model
reward_model_adapters: ""  # Adapters for reward model
reward_model_quantization_bit: 8  # Number of bits for quantization of reward model
reward_model_type: "lora"  # Type of reward model ('lora', 'full', 'api')
additional_target: ""  # Additional target
lora_rank: 16  # Rank for LoRA
lora_alpha: 32  # Alpha for LoRA
lora_dropout: 0.05  # Dropout rate for LoRA
lora_target: "q_proj,v_proj"  # Target for LoRA
loraplus_lr_ratio: 0.1  # Learning rate ratio for LoRA+
loraplus_lr_embedding: 0.01  # Learning rate for embedding in LoRA+
use_rslora: false  # Use RSLORA
use_dora: false  # Use DORA
create_new_adapter: false  # Create new adapter
freeze_trainable_layers: ""  # Layers to freeze during training
freeze_trainable_modules: ""  # Modules to freeze during training
freeze_extra_modules: ""  # Extra modules to freeze
pure_bf16: false  # Use pure bfloat16 precision
stage: "pt"  # Stage of training ('pt', 'sft', 'rm', 'ppo', 'dpo', 'kto', 'orpo')
finetuning_type: "lora"  # Type of finetuning ('lora', 'freeze', 'full')
use_llama_pro: false  # Use LLaMA Pro
plot_loss: true  # Plot loss during training
do_sample: false  # Enable sampling during generation
temperature: 1.0  # Temperature for sampling
top_p: 0.9  # Top-p value for nucleus sampling
top_k: 50  # Top-k value for sampling
num_beams: 5  # Number of beams for beam search
max_length: 1024  # Maximum length for generated sequences
max_new_tokens: 50  # Maximum number of new tokens to generate
repetition_penalty: 1.0  # Repetition penalty for generation
length_penalty: 1.0  # Length penalty for generation
default_system: ""  # Default system for generation
